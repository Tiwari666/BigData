{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tiwari666/BigData/blob/main/big_data_pyspark_RDD_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main documnet:https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
        "\n",
        "Main Data source: https://www.kaggle.com/Cornell-University/arxiv/version/62"
      ],
      "metadata": {
        "id": "cAbTbuAKSLao"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Diqf_I3S7YX",
        "outputId": "1c146490-2144-49cb-bba7-4dfdb33f3501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Install Kaggle CLI\n",
        "!pip install kaggle\n",
        "\n",
        "# Install PySpark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6buaOnGeToeG"
      },
      "source": [
        " **Google Colab** is a cloud-based environment that does not have direct access to our Kaggle account or files. To download datasets from Kaggle into Colab, you need to use the Kaggle CLI, which acts as a bridge between our Kaggle account and Colab. The Kaggle CLI is a tool that allows us to:\n",
        "\n",
        "--Authenticate our Kaggle account using an API key (kaggle.json).\n",
        "\n",
        "--Search and download datasets from Kaggle directly into Colab.\n",
        "\n",
        "--Perform tasks like submitting to competitions or listing available datasets.\n",
        "\n",
        "Without the Kaggle CLI, we would need to manually download the dataset to our local machine and upload it to Colab, which is time-consuming for large files.\n",
        "\n",
        "**Simple Exmaple**\n",
        "\n",
        "We want to order pizza from our favorite restaurant. We can either:\n",
        "\n",
        "--Call the restaurant directly and place our order (CLI approach, fast and efficient).\n",
        "\n",
        "--Walk to the restaurant, place our order, and wait for it (manual download/upload, slow and tedious).\n",
        "\n",
        "--The Kaggle CLI is like calling the restaurant—it lets us access Kaggle directly from Colab and quickly download our data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSn-Bsmcewi_"
      },
      "source": [
        "**Step 1: Generate Kaggle API Token**\n",
        "\n",
        "--To create the kaggle.json file:\n",
        "\n",
        "--Visit the Kaggle Account Settings page.\n",
        "\n",
        "--Scroll down to the API section.\n",
        "\n",
        "--Click on \"Create New API Token\".\n",
        "\n",
        "This will download a file named kaggle.json to our computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlYM7LOKURyP"
      },
      "source": [
        "**Step 2: Upload kaggle.json to Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "DFXWcgz7VUcG",
        "outputId": "3777060d-c424-4b46-aa87-3264e6fb6ace"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9e9f7534-ae76-4a67-a247-f53024ab83a1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9e9f7534-ae76-4a67-a247-f53024ab83a1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"narendratiwari1\",\"key\":\"6f6ff19aec2f64f50c4ea2a53f678434\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # This will open a dialog to upload files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21K50cEZfOcP"
      },
      "source": [
        "**Step 3: Configure Kaggle in Google Colab**\n",
        "\n",
        "Move the kaggle.json file to the .kaggle directory and set proper permissions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZCNmNSHvfVJ_"
      },
      "outputs": [],
      "source": [
        "# Create the .kaggle directory\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Move the kaggle.json file to the .kaggle directory\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set the appropriate permissions\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK9rwCnVfeC4"
      },
      "source": [
        "**Step 4: Verify Kaggle CLI**\n",
        "\n",
        "Test the Kaggle CLI to ensure it's set up correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW2N4zQifeFg",
        "outputId": "e276e938-be52-4300-8c68-578be666b0cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                         title                                             size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "----------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "bhadramohit/customer-shopping-latest-trends-dataset         Customer Shopping (Latest Trends) Dataset         76KB  2024-11-23 15:26:12          19153        377  1.0              \n",
            "hopesb/student-depression-dataset                           Student Depression Dataset.                      454KB  2024-11-22 17:56:03          15676        222  1.0              \n",
            "oktayrdeki/houses-in-london                                 Houses in London                                  21KB  2024-12-15 19:27:42           1416         27  1.0              \n",
            "mhassansaboor/intel-stock-data-1980-2024                    Intel Stock Data (1980-2024)                     281KB  2024-12-25 16:12:36            656         24  1.0              \n",
            "michaellanurias/spotify-playlist-origins                    Spotify Playlist-ORIGINS                          25KB  2024-12-15 01:16:34            905         22  1.0              \n",
            "kanchana1990/global-adult-hiv-prevalance-data-2024-updated  Global Adult HIV Prevalance Data (2024 Updated)    3KB  2024-12-28 17:32:21            322         24  1.0              \n",
            "bushraqurban/world-health-indicators-dataset                🏥 Global Health Indicators Dataset 📊             186KB  2024-12-22 02:35:39            897         26  1.0              \n",
            "taimoor888/top-100-youtube-channels-in-2024                 Top 100 YouTube Channels in 2024                   3KB  2024-12-15 16:09:25           1174         34  1.0              \n",
            "sezginfurkan/geophone-sensor-dataset                        Geophone Sensor Dataset                           74KB  2024-12-26 18:23:21            323         22  1.0              \n",
            "denkuznetz/food-delivery-time-prediction                    Food Delivery Time Prediction 🛵                   12KB  2024-12-23 13:12:14           1107         35  1.0              \n",
            "mujtabamatin/air-quality-and-pollution-assessment           Air Quality and Pollution Assessment              84KB  2024-12-04 15:29:51           7402        115  1.0              \n",
            "bushraqurban/tourism-and-economic-impact                    ✈️ Tourism and Economic Impact Dataset💰          270KB  2024-12-22 08:47:37           1303         34  1.0              \n",
            "kanchana1990/world-internet-usage-data-2023-updated         World Internet Usage Data (2023 Updated)           4KB  2024-12-21 09:41:41            643         43  1.0              \n",
            "rahmasleam/breast-cancer                                    Breast Cancer                                     49KB  2024-12-10 08:44:26           1332         33  1.0              \n",
            "anandshaw2001/chatgpt-users-reviews                         ChatGPT Users Reviews                              9MB  2024-12-26 10:14:42            378         25  1.0              \n",
            "gauthamvijayaraj/spotify-tracks-dataset-updated-every-week  Spotify Tracks Dataset (Updated every week)        5MB  2024-12-09 18:00:01           3189         62  1.0              \n",
            "dansbecker/melbourne-housing-snapshot                       Melbourne Housing Snapshot                       451KB  2018-06-05 12:52:24         167136       1557  0.7058824        \n",
            "denkuznetz/traffic-accident-prediction                      Traffic Accident Prediction 💥🚗                    10KB  2024-12-11 11:04:47           2447         44  1.0              \n",
            "steve1215rogg/e-commerce-dataset                            E-Commerce Dataset                                90KB  2024-11-22 22:10:02           6551         78  1.0              \n",
            "vedaantsingh/comprehensive-cryptocurrency-market-data       Comprehensive Cryptocurrency Market Data 🚀🌕      755KB  2024-12-22 05:32:22            671         24  1.0              \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets list\n",
        "#This will display a list of available datasets on Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuxrOtTEfeH_"
      },
      "source": [
        "**Step 5: Search for the Desired Dataset**\n",
        "\n",
        "Download the arXiv Dataset\n",
        "Run the following command to download the arXiv Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1rarnk1kK3A",
        "outputId": "dde8ac7a-f948-4b8f-95fb-96d460488ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/Cornell-University/arxiv\n",
            "License(s): CC0-1.0\n",
            "Downloading arxiv.zip to /content\n",
            " 99% 1.37G/1.38G [00:10<00:00, 162MB/s]\n",
            "100% 1.38G/1.38G [00:10<00:00, 137MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d Cornell-University/arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdzZ9huMk8fJ"
      },
      "source": [
        "**Unzip the Dataset**\n",
        "\n",
        "Extract the contents of the downloaded zip file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxHmX4utk8hw",
        "outputId": "ba1db958-8406-496c-d397-0070949b2e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  arxiv.zip\n",
            "  inflating: arxiv-metadata-oai-snapshot.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip arxiv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Data**\n",
        "\n",
        "Since the dataset is now unzipped, load it into a Spark RDD."
      ],
      "metadata": {
        "id": "XBHBZsd0s1mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import os"
      ],
      "metadata": {
        "id": "wtBuKrGOsvLU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set JAVA_HOME if not set\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "xoIURm-bsvTm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify Java Installation**\n",
        "\n",
        "Ensure Java is properly installed and set up in Google Colab."
      ],
      "metadata": {
        "id": "ePuBQTICtiI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] += \":/usr/lib/jvm/java-8-openjdk-amd64/bin\""
      ],
      "metadata": {
        "id": "RTmPpVmitXYI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install PySpark**\n",
        "\n",
        "Install the required version of PySpark in Colab."
      ],
      "metadata": {
        "id": "v6pUhJYstnhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SnHmTHjtxUC",
        "outputId": "abe6c642-1720-4cf7-d3b7-2d944a129ced"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure SparkSession**"
      ],
      "metadata": {
        "id": "TzXbvineubh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Arxiv Dataset Analysis\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"SparkSession started successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OHRi9JcuUag",
        "outputId": "727e69a0-b757-4a7d-e3dc-e06bdc88f006"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession started successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Data**"
      ],
      "metadata": {
        "id": "Xaf3GuiQuzPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load JSON file as RDD\n",
        "rdd = spark.read.json(\"/content/arxiv-metadata-oai-snapshot.json\").rdd\n",
        "print(f\"RDD successfully created with {rdd.count()} rows.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt31yk05svY4",
        "outputId": "3cd08b5d-3831-492d-a4a3-b8c7666a304b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD successfully created with 2635119 rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation Operations**\n",
        "\n",
        "Transformations are operations on RDDs that return another RDD. They are lazy, meaning they are not executed until an action is called."
      ],
      "metadata": {
        "id": "C2nydXuiw8O4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filter Operation**\n",
        "\n",
        "Filter rows to find those that belong to a specific category (e.g., \"physics\"):"
      ],
      "metadata": {
        "id": "iaJu996Iuv7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physics_rdd = rdd.filter(lambda row: \"physics\" in row[\"categories\"])\n",
        "print(f\"Filtered RDD with rows related to physics created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3UczJXguv-P",
        "outputId": "68afb38c-e6af-4628-da97-d126eea8cd5c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered RDD with rows related to physics created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Map Operation**\n",
        "\n",
        "Extract specific fields (e.g., \"title\") from the dataset:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Vy1ZUkBxKrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titles_rdd = rdd.map(lambda row: row[\"title\"])\n",
        "print(f\"Mapped RDD created with only titles.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzVAO0QnuwDK",
        "outputId": "77ae2700-58e1-486c-9944-797efe0a73b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped RDD created with only titles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FlatMap Operation**\n",
        "\n",
        "Split categories and flatten them into individual entries:"
      ],
      "metadata": {
        "id": "Ppac2MNZyHWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories_rdd = rdd.flatMap(lambda row: row[\"categories\"].split())\n",
        "print(f\"FlatMapped RDD created for categories.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM1piFsfuwFr",
        "outputId": "a82d54e4-4bb4-4474-c7b6-ddae4b561fa6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FlatMapped RDD created for categories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distinct Operation**\n",
        "\n",
        "Get unique categories:"
      ],
      "metadata": {
        "id": "JijzSauyuwIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_categories_rdd = categories_rdd.distinct()\n",
        "print(f\"Distinct RDD created with unique categories.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la55dZ8_svbd",
        "outputId": "3c19f987-5a3b-4ad5-d06c-a1bd2fd23419"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct RDD created with unique categories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Operation**\n",
        "\n",
        "Sample 1% of the data for quick computation:"
      ],
      "metadata": {
        "id": "uCuPisQpsvdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_rdd = rdd.sample(False, 0.01, seed=42)\n",
        "print(f\"Sampled RDD with 1% of the data created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqJWfP5Ssvf9",
        "outputId": "34daf27d-513e-462f-abb9-219f39d4062e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled RDD with 1% of the data created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Action Operations**\n",
        "\n",
        "Actions are operations that trigger computation. They return results to the driver or save results to storage.\n",
        "\n",
        "**Count Operation**\n",
        "\n",
        "Count the total number of rows in the RDD:"
      ],
      "metadata": {
        "id": "2smHqY-Csvic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = rdd.count()\n",
        "print(f\"Total rows in the RDD: {total_rows}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wayeVC6svkL",
        "outputId": "fd66ca22-be3b-44eb-bb3b-f6a63f55175e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in the RDD: 2635119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Take Operation**\n",
        "\n",
        "Get the first 5 rows of the RDD:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rn7tA_CG0xA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rows = rdd.take(5)\n",
        "print(f\"Sample rows: {sample_rows}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxpa3pV9svmw",
        "outputId": "81457d27-8044-4af5-a29d-28fdd727fcfa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample rows: [Row(abstract='  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n', authors=\"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\", authors_parsed=[['Balázs', 'C.', ''], ['Berger', 'E. L.', ''], ['Nadolsky', 'P. M.', ''], ['Yuan', 'C. -P.', '']], categories='hep-ph', comments='37 pages, 15 figures; published version', doi='10.1103/PhysRevD.76.013009', id='0704.0001', journal-ref='Phys.Rev.D76:013009,2007', license=None, report-no='ANL-HEP-PR-07-12', submitter='Pavel Nadolsky', title='Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies', update_date='2008-11-26', versions=[Row(created='Mon, 2 Apr 2007 19:18:42 GMT', version='v1'), Row(created='Tue, 24 Jul 2007 20:10:27 GMT', version='v2')]), Row(abstract='  We describe a new algorithm, the $(k,\\\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n', authors='Ileana Streinu and Louis Theran', authors_parsed=[['Streinu', 'Ileana', ''], ['Theran', 'Louis', '']], categories='math.CO cs.CG', comments='To appear in Graphs and Combinatorics', doi=None, id='0704.0002', journal-ref=None, license='http://arxiv.org/licenses/nonexclusive-distrib/1.0/', report-no=None, submitter='Louis Theran', title='Sparsity-certifying Graph Decompositions', update_date='2008-12-13', versions=[Row(created='Sat, 31 Mar 2007 02:26:18 GMT', version='v1'), Row(created='Sat, 13 Dec 2008 17:26:00 GMT', version='v2')]), Row(abstract=\"  The evolution of Earth-Moon system is described by the dark matter field\\nfluid model proposed in the Meeting of Division of Particle and Field 2004,\\nAmerican Physical Society. The current behavior of the Earth-Moon system agrees\\nwith this model very well and the general pattern of the evolution of the\\nMoon-Earth system described by this model agrees with geological and fossil\\nevidence. The closest distance of the Moon to Earth was about 259000 km at 4.5\\nbillion years ago, which is far beyond the Roche's limit. The result suggests\\nthat the tidal friction may not be the primary cause for the evolution of the\\nEarth-Moon system. The average dark matter field fluid constant derived from\\nEarth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts\\nthat the Mars's rotation is also slowing with the angular acceleration rate\\nabout -4.38 x 10^(-22) rad s^(-2).\\n\", authors='Hongjun Pan', authors_parsed=[['Pan', 'Hongjun', '']], categories='physics.gen-ph', comments='23 pages, 3 figures', doi=None, id='0704.0003', journal-ref=None, license=None, report-no=None, submitter='Hongjun Pan', title='The evolution of the Earth-Moon system based on the dark matter field\\n  fluid model', update_date='2008-01-13', versions=[Row(created='Sun, 1 Apr 2007 20:46:54 GMT', version='v1'), Row(created='Sat, 8 Dec 2007 23:47:24 GMT', version='v2'), Row(created='Sun, 13 Jan 2008 00:36:28 GMT', version='v3')]), Row(abstract='  We show that a determinant of Stirling cycle numbers counts unlabeled acyclic\\nsingle-source automata. The proof involves a bijection from these automata to\\ncertain marked lattice paths and a sign-reversing involution to evaluate the\\ndeterminant.\\n', authors='David Callan', authors_parsed=[['Callan', 'David', '']], categories='math.CO', comments='11 pages', doi=None, id='0704.0004', journal-ref=None, license=None, report-no=None, submitter='David Callan', title='A determinant of Stirling cycle numbers counts unlabeled acyclic\\n  single-source automata', update_date='2007-05-23', versions=[Row(created='Sat, 31 Mar 2007 03:16:14 GMT', version='v1')]), Row(abstract='  In this paper we show how to compute the $\\\\Lambda_{\\\\alpha}$ norm, $\\\\alpha\\\\ge\\n0$, using the dyadic grid. This result is a consequence of the description of\\nthe Hardy spaces $H^p(R^N)$ in terms of dyadic and special atoms.\\n', authors='Wael Abu-Shammala and Alberto Torchinsky', authors_parsed=[['Abu-Shammala', 'Wael', ''], ['Torchinsky', 'Alberto', '']], categories='math.CA math.FA', comments=None, doi=None, id='0704.0005', journal-ref='Illinois J. Math. 52 (2008) no.2, 681-689', license=None, report-no=None, submitter='Alberto Torchinsky', title='From dyadic $\\\\Lambda_{\\\\alpha}$ to $\\\\Lambda_{\\\\alpha}$', update_date='2013-10-15', versions=[Row(created='Mon, 2 Apr 2007 18:09:58 GMT', version='v1')])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collect Operation**\n",
        "\n",
        "Collect and print the first 10 unique categories"
      ],
      "metadata": {
        "id": "hGzio2I40_4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 1% of the RDD for safe computation\n",
        "sampled_rdd = rdd.sample(False, 0.01, seed=42)\n",
        "\n",
        "# Extract and get distinct categories from the sampled RDD\n",
        "sampled_categories_rdd = sampled_rdd.flatMap(lambda row: row[\"categories\"].split()).distinct()\n",
        "\n",
        "# Collect and print the first 10 unique categories from the sampled data\n",
        "unique_categories = sampled_categories_rdd.take(10)\n",
        "print(f\"Unique categories (from sample): {unique_categories}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi076X-NsvpN",
        "outputId": "ef1f479e-ad9c-4bcf-b86f-6dd962fdbb8a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique categories (from sample): ['q-bio.QM', 'physics.atom-ph', 'physics.geo-ph', 'math.GM', 'cs.CL', 'q-fin.RM', 'astro-ph', 'math.QA', 'math.CA', 'math.MG']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Action Operations**\n",
        "\n",
        "Actions are operations that trigger computation. They return results to the driver or save results to storage.\n",
        "\n",
        "**Count Operation**\n",
        "\n",
        "Count the total number of rows in the RDD:"
      ],
      "metadata": {
        "id": "iJV_SO3A3CGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the total rows in the sampled RDD\n",
        "sampled_count = sampled_rdd.count()\n",
        "print(f\"Total rows in the sampled RDD: {sampled_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfWoWGKE1tC_",
        "outputId": "c3434505-089c-44ba-8250-50ea389e5cba"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in the sampled RDD: 26128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count the Occurrences of Each Category**\n",
        "\n",
        "This will show how many papers belong to each category in the sampled data."
      ],
      "metadata": {
        "id": "qXAwhzqQ32Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map each category to (category, 1) and reduce by key to count occurrences\n",
        "category_counts_rdd = sampled_categories_rdd.map(lambda category: (category, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect and display the counts for a few categories\n",
        "category_counts = category_counts_rdd.take(10)  # Fetch the first 10 categories for display\n",
        "print(f\"Category counts (sampled): {category_counts}\")"
      ],
      "metadata": {
        "id": "t-AQtsH21tGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filter Papers Related to Specific Categories**\n",
        "\n",
        "For example, filter papers with the \"physics\" category."
      ],
      "metadata": {
        "id": "G5POWwWQ5Ckp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter papers containing the \"physics\" category in their categories field\n",
        "physics_papers_rdd = sampled_rdd.filter(lambda row: \"physics\" in row[\"categories\"])\n",
        "\n",
        "# Count and display a few sample rows of physics-related papers\n",
        "physics_papers_count = physics_papers_rdd.count()\n",
        "sample_physics_papers = physics_papers_rdd.take(5)\n",
        "print(f\"Total physics-related papers in the sample: {physics_papers_count}\")\n",
        "print(f\"Sample physics-related papers: {sample_physics_papers}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI9gX8SC1tNA",
        "outputId": "c71da8b4-318c-4332-bf75-9fab39c621f2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total physics-related papers in the sample: 2650\n",
            "Sample physics-related papers: [Row(abstract='  After the pioneered experimental works on superlubricity by Martin et al. on\\nMoS2 [1], Hirano et al. on tungsten and silicon [2] and the further\\nconfirmation by Dienwiebel et al. on graphite [3], many groups around the word\\ninvestigated the occurrence of near frictionless sliding contacts. This large\\nmobilization of tribologists, material sciences specialists and physicists has\\nlead to emerging solutions involving new materials and coatings, the most\\npromising being carbon based like graphite, diamond, carbon composites or\\ndiamond-like-carbons. Some of them are currently used in practical\\napplications. The situation is different especially in EHL: the highest\\nfriction coefficients are close to 10% when traction fluids are involved, i.e.\\nfluids that have especially designed to transmit the highest friction, and they\\nvary within 3-6% for the rest of lubricants. The range of variation is\\nconsequently very narrow and these typical values are really low compared to\\nthose obtained in dry contacts: as a consequence the gain expected from a super\\nlow traction regime in lubrication will be probably more limited, especially in\\nthe case of experiments conducted at the meso or macro scales. This weak\\nperspective could be one explanation on the relatively low number of articles\\nin recent literature dealing with lubricated superlubricity in the above\\nconditions.\\n', authors='Philippe Vergne (LaMCoS)', authors_parsed=[['Vergne', 'Philippe', '', 'LaMCoS']], categories='physics.class-ph', comments=None, doi=None, id='0704.1799', journal-ref='Superlubricity, Elsevier BV (Ed.) (31/03/2007) 429-445', license=None, report-no=None, submitter='Philippe Vergne', title='Super Low Traction under EHD and Mixed Lubrication Regimes', update_date='2007-05-23', versions=[Row(created='Fri, 13 Apr 2007 17:05:21 GMT', version='v1')]), Row(abstract='  Using the results found previously for the cooling rates of the emittances,\\ndue to collisions between the electrons and the ions, a result is found for the\\nfriction force acting on the ions. It is shown that the friction force found\\nhere when used to track the ion bunch will give the same emittance cooling\\nrates as those found using the intrabeam scattering theory for electron cooling\\n>.For the case of the uniform in space electron bunch distribution, the\\nfriction force found here agrees with the friction force result found with the\\nusual theory of electron cooling.\\n', authors='George Parzen', authors_parsed=[['Parzen', 'George', '']], categories='physics.acc-ph', comments='12 pages', doi=None, id='0705.0339', journal-ref=None, license=None, report-no='C-A/AP/#261, November 2006', submitter='George Parzen', title='Theory of the friction force using electron cooling as an intrabeam\\n  scattering process', update_date='2007-05-23', versions=[Row(created='Wed, 2 May 2007 18:05:18 GMT', version='v1')]), Row(abstract=\"  An easy-to-implement form of the Metropolis Algorithm is described which,\\nunlike most standard techniques, is well suited to sampling from multi-modal\\ndistributions on spaces with moderate numbers of dimensions (order ten) in\\nenvironments typical of investigations into current constraints on\\nBeyond-the-Standard-Model physics. The sampling technique makes use of\\npre-existing information (which can safely be of low or uncertain quality)\\nrelating to the distribution from which it is desired to sample. This\\ninformation should come in the form of a ``bank'' or ``cache'' of space points\\nof which at least some may be expected to be near regions of interest in the\\ndesired distribution. In practical circumstances such ``banks of clues'' are\\neasy to assemble from earlier work, aborted runs, discarded burn-in samples\\nfrom failed sampling attempts, or from prior scouting investigations. The\\ntechnique equilibrates between disconnected parts of the distribution without\\nuser input. The algorithm is not lead astray by ``bad'' clues, but there is no\\nfree lunch: performance gains will only be seen where clues are helpful.\\n\", authors='Benjamin C. Allanach, Christopher G. Lester', authors_parsed=[['Allanach', 'Benjamin C.', ''], ['Lester', 'Christopher G.', '']], categories='hep-ph physics.data-an', comments='v1: 18 pages, 7 figures. v2: 22 pages, 9 figures: no changes to the\\n  algorithm, but more example distributions are provided against which the\\n  sampler is tested', doi='10.1016/j.cpc.2008.02.020', id='0705.0486', journal-ref='Comput.Phys.Commun.179:256-266,2008', license=None, report-no='DAMTP-2007-18, Cavendish-HEP-2007-02', submitter='Christopher Gorham Lester', title=\"Sampling using a `bank' of clues\", update_date='2008-11-26', versions=[Row(created='Thu, 3 May 2007 15:50:08 GMT', version='v1'), Row(created='Mon, 26 Nov 2007 21:59:48 GMT', version='v2')]), Row(abstract='  We study the effects of different forms of information feedback associated\\nwith mass media on an agent-agent based model of the dynamics of cultural\\ndissemination. In addition to some processes previously considered, we also\\nexamine a model of local mass media influence in cultural dynamics. Two\\nmechanisms of information feedback are investigated: (i) direct mass media\\ninfluence, where local or global mass media act as an additional element in the\\nnetwork of interactions of each agent, and (ii) indirect mass media influence,\\nwhere global media acts as a filter of the influence of the existing network of\\ninteractions of each agent. Our results generalize previous findings showing\\nthat cultural diversity builds-up by increasing the strength of the mass media\\ninfluence. We find that this occurs independently of the mechanisms of action\\n(direct or indirect) of the mass media message. However, through an analysis of\\nthe full range of parameters measuring cultural diversity, we establish that\\nthe enhancement of cultural diversity produced by interaction with mass media\\nonly occurs for strong enough mass media messages. In comparison with previous\\nstudies a main different result is that weak mass media messages, in\\ncombination with agent-agent interaction, are efficient in producing cultural\\nhomogeneity. Moreover, the homogenizing effect of weak mass media messages are\\nmore efficient for direct local mass media messages than for global mass media\\nmessages or indirect global mass media influences.\\n', authors='J.C. Gonzalez-Avella, M.G. Cosenza, K. Klemm, V.M. Eguiluz and M. San\\n  Miguel', authors_parsed=[['Gonzalez-Avella', 'J. C.', ''], ['Cosenza', 'M. G.', ''], ['Klemm', 'K.', ''], ['Eguiluz', 'V. M.', ''], ['Miguel', 'M. San', '']], categories='physics.soc-ph', comments='20n pages, 10 figures', doi=None, id='0705.1091', journal-ref='Journal of Artificial Societies and Social Simulation 10 (3), 9\\n  (2007)', license=None, report-no='https://www.jasss.org/10/3/9.html', submitter='Juan Carlos Gonzalez Avella Md.', title='Information feedback and mass media effects in cultural dynamics', update_date='2024-11-05', versions=[Row(created='Tue, 8 May 2007 14:38:41 GMT', version='v1')]), Row(abstract='  We observe experimentally higher-order solitons in waveguide arrays with\\ndefocusing saturable nonlinearity. Such solitons can comprise several in-phase\\nbright spots and are stable above a critical power threshold. We elucidate the\\nimpact of the nonlinearity saturation on the domains of existence and stability\\nof the observed complex soliton states.\\n', authors='Eugene Smirnov, Christian E. Ruter, Detlef Kip, Yaroslav V. Kartashov,\\n  Lluis Torner', authors_parsed=[['Smirnov', 'Eugene', ''], ['Ruter', 'Christian E.', ''], ['Kip', 'Detlef', ''], ['Kartashov', 'Yaroslav V.', ''], ['Torner', 'Lluis', '']], categories='physics.optics nlin.PS', comments='13 pages, 3 figures, to appear in Optics Letters', doi='10.1364/OL.32.001950', id='0705.1704', journal-ref='Optics Letters 32, 1950 (2007)', license=None, report-no=None, submitter='Yaroslav Kartashov', title='Observation of higher-order solitons in defocusing waveguide arrays', update_date='2009-11-13', versions=[Row(created='Fri, 11 May 2007 17:55:55 GMT', version='v1')])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group Papers by Year**\n",
        "\n",
        "This will group papers based on their publication year (if available)."
      ],
      "metadata": {
        "id": "eEuKVnJw5SWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract year from the \"update_date\" field and group papers by year\n",
        "year_papers_rdd = sampled_rdd.map(lambda row: (row[\"update_date\"][:4], 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect and display the counts for a few years\n",
        "year_paper_counts = year_papers_rdd.take(10)  # Fetch first 10 years for display\n",
        "print(f\"Papers grouped by year (sampled): {year_paper_counts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aXJuLpm1tPw",
        "outputId": "a34f10f9-0178-45ff-cd61-b17b85d03aca"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Papers grouped by year (sampled): [('2011', 607), ('2020', 1769), ('2007', 1436), ('2016', 1345), ('2017', 1256), ('2008', 657), ('2021', 1804), ('2015', 2050), ('2014', 845), ('2024', 3173)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Persist Intermediate Results**\n",
        "\n",
        "Cache the sampled RDD to avoid recomputation during repeated operations.\n",
        "\n",
        "Caching or persisting the RDD saves intermediate results in memory, so Spark doesn't repeat the same calculations every time when we run an operation.\n",
        "\n",
        "**Why do it?**\n",
        "\n",
        "--Faster Analysis: Avoids recomputing data for repeated actions.\n",
        "\n",
        "--Prevents Crashes: Reduces workload, especially for large datasets.\n",
        "\n",
        "--Efficient Workflow: Saves time when performing multiple operations on the same data.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "It speeds up our workflow and makes analysis smooth and stable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5zhgkvQW7pcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache the sampled RDD\n",
        "sampled_rdd.cache()\n",
        "print(\"Sampled RDD cached for reuse.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mvggyC31tR6",
        "outputId": "8f4c3760-6868-47ab-f331-03180bfed026"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled RDD cached for reuse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check Number of Partitions**"
      ],
      "metadata": {
        "id": "n3_AkgPw9LcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_partitions = rdd.getNumPartitions()\n",
        "print(f\"Number of partitions in the RDD: {num_partitions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7JOV7yI1tUh",
        "outputId": "8c0c65db-6425-4b49-90d5-4639b292bac4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions in the RDD: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Preparation**"
      ],
      "metadata": {
        "id": "cY7csTQz9bke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = rdd.toDF()\n",
        "print(f\"Converted RDD to DataFrame for ML analysis.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZKMhzTxsvr6",
        "outputId": "5ed43fab-3fa1-48a6-a12a-4b2ca38b5f49"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted RDD to DataFrame for ML analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the Schema**\n",
        "\n",
        "Inspect the structure of the DataFrame to understand its columns and data types."
      ],
      "metadata": {
        "id": "RiTzUpTI9x8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the schema of the DataFrame\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVxWLrpysvuK",
        "outputId": "1b84e76f-0e7b-4043-947e-a249b20a4f96"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- abstract: string (nullable = true)\n",
            " |-- authors: string (nullable = true)\n",
            " |-- authors_parsed: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- categories: string (nullable = true)\n",
            " |-- comments: string (nullable = true)\n",
            " |-- doi: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- journal-ref: string (nullable = true)\n",
            " |-- license: string (nullable = true)\n",
            " |-- report-no: string (nullable = true)\n",
            " |-- submitter: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- update_date: string (nullable = true)\n",
            " |-- versions: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- created: string (nullable = true)\n",
            " |    |    |-- version: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore the Data**\n",
        "\n",
        "View a sample of the data to ensure it looks as expected."
      ],
      "metadata": {
        "id": "ReI1_0UW965D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first few rows of the DataFrame\n",
        "df.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32LkfXsrsvwi",
        "outputId": "0cdce87a-27fe-4d30-9655-3c0ded4fed9b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------+--------------------------------------------------------------------------+---------------+---------------------------------------+--------------------------+---------+-----------------------------------------+---------------------------------------------------+----------------+------------------+------------------------------------------------------------------------------------------+-----------+-------------------------------------------------------------------------------------------------------------+\n",
            "|abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |authors                                              |authors_parsed                                                            |categories     |comments                               |doi                       |id       |journal-ref                              |license                                            |report-no       |submitter         |title                                                                                     |update_date|versions                                                                                                     |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------+--------------------------------------------------------------------------+---------------+---------------------------------------+--------------------------+---------+-----------------------------------------+---------------------------------------------------+----------------+------------------+------------------------------------------------------------------------------------------+-----------+-------------------------------------------------------------------------------------------------------------+\n",
            "|  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n|C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan|[[Balázs, C., ], [Berger, E. L., ], [Nadolsky, P. M., ], [Yuan, C. -P., ]]|hep-ph         |37 pages, 15 figures; published version|10.1103/PhysRevD.76.013009|0704.0001|Phys.Rev.D76:013009,2007                 |NULL                                               |ANL-HEP-PR-07-12|Pavel Nadolsky    |Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies  |2008-11-26 |[{Mon, 2 Apr 2007 19:18:42 GMT, v1}, {Tue, 24 Jul 2007 20:10:27 GMT, v2}]                                    |\n",
            "|  We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n                                                                                                                                                                                            |Ileana Streinu and Louis Theran                      |[[Streinu, Ileana, ], [Theran, Louis, ]]                                  |math.CO cs.CG  |To appear in Graphs and Combinatorics  |NULL                      |0704.0002|NULL                                     |http://arxiv.org/licenses/nonexclusive-distrib/1.0/|NULL            |Louis Theran      |Sparsity-certifying Graph Decompositions                                                  |2008-12-13 |[{Sat, 31 Mar 2007 02:26:18 GMT, v1}, {Sat, 13 Dec 2008 17:26:00 GMT, v2}]                                   |\n",
            "|  The evolution of Earth-Moon system is described by the dark matter field\\nfluid model proposed in the Meeting of Division of Particle and Field 2004,\\nAmerican Physical Society. The current behavior of the Earth-Moon system agrees\\nwith this model very well and the general pattern of the evolution of the\\nMoon-Earth system described by this model agrees with geological and fossil\\nevidence. The closest distance of the Moon to Earth was about 259000 km at 4.5\\nbillion years ago, which is far beyond the Roche's limit. The result suggests\\nthat the tidal friction may not be the primary cause for the evolution of the\\nEarth-Moon system. The average dark matter field fluid constant derived from\\nEarth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts\\nthat the Mars's rotation is also slowing with the angular acceleration rate\\nabout -4.38 x 10^(-22) rad s^(-2).\\n                                                                                                         |Hongjun Pan                                          |[[Pan, Hongjun, ]]                                                        |physics.gen-ph |23 pages, 3 figures                    |NULL                      |0704.0003|NULL                                     |NULL                                               |NULL            |Hongjun Pan       |The evolution of the Earth-Moon system based on the dark matter field\\n  fluid model      |2008-01-13 |[{Sun, 1 Apr 2007 20:46:54 GMT, v1}, {Sat, 8 Dec 2007 23:47:24 GMT, v2}, {Sun, 13 Jan 2008 00:36:28 GMT, v3}]|\n",
            "|  We show that a determinant of Stirling cycle numbers counts unlabeled acyclic\\nsingle-source automata. The proof involves a bijection from these automata to\\ncertain marked lattice paths and a sign-reversing involution to evaluate the\\ndeterminant.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |David Callan                                         |[[Callan, David, ]]                                                       |math.CO        |11 pages                               |NULL                      |0704.0004|NULL                                     |NULL                                               |NULL            |David Callan      |A determinant of Stirling cycle numbers counts unlabeled acyclic\\n  single-source automata|2007-05-23 |[{Sat, 31 Mar 2007 03:16:14 GMT, v1}]                                                                        |\n",
            "|  In this paper we show how to compute the $\\Lambda_{\\alpha}$ norm, $\\alpha\\ge\\n0$, using the dyadic grid. This result is a consequence of the description of\\nthe Hardy spaces $H^p(R^N)$ in terms of dyadic and special atoms.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Wael Abu-Shammala and Alberto Torchinsky             |[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]                         |math.CA math.FA|NULL                                   |NULL                      |0704.0005|Illinois J. Math. 52 (2008) no.2, 681-689|NULL                                               |NULL            |Alberto Torchinsky|From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\alpha}$                                      |2013-10-15 |[{Mon, 2 Apr 2007 18:09:58 GMT, v1}]                                                                         |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------+--------------------------------------------------------------------------+---------------+---------------------------------------+--------------------------+---------+-----------------------------------------+---------------------------------------------------+----------------+------------------+------------------------------------------------------------------------------------------+-----------+-------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List all column names\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3k_PY9_sv04",
        "outputId": "abb62b4c-3c94-4e14-b282-b491c92b3180"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abstract', 'authors', 'authors_parsed', 'categories', 'comments', 'doi', 'id', 'journal-ref', 'license', 'report-no', 'submitter', 'title', 'update_date', 'versions']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first 10 rows of the DataFrame\n",
        "df.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt2nZFzF-0oT",
        "outputId": "90cf60b8-3b3a-409a-f78f-291edd81dafb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------------------------------------------------------+--------------------------+---------+--------------------------------------------------+---------------------------------------------------+-------------------------------+------------------+-------------------------------------------------------------------------------------------------------------------------------+-----------+--------------------------------------------------------------------------------------------------------------+\n",
            "|abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |authors                                                                                                          |authors_parsed                                                                                                                                    |categories       |comments                                                        |doi                       |id       |journal-ref                                       |license                                            |report-no                      |submitter         |title                                                                                                                          |update_date|versions                                                                                                      |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------------------------------------------------------+--------------------------+---------+--------------------------------------------------+---------------------------------------------------+-------------------------------+------------------+-------------------------------------------------------------------------------------------------------------------------------+-----------+--------------------------------------------------------------------------------------------------------------+\n",
            "|  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan                                                            |[[Balázs, C., ], [Berger, E. L., ], [Nadolsky, P. M., ], [Yuan, C. -P., ]]                                                                        |hep-ph           |37 pages, 15 figures; published version                         |10.1103/PhysRevD.76.013009|0704.0001|Phys.Rev.D76:013009,2007                          |NULL                                               |ANL-HEP-PR-07-12               |Pavel Nadolsky    |Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies                                       |2008-11-26 |[{Mon, 2 Apr 2007 19:18:42 GMT, v1}, {Tue, 24 Jul 2007 20:10:27 GMT, v2}]                                     |\n",
            "|  We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |Ileana Streinu and Louis Theran                                                                                  |[[Streinu, Ileana, ], [Theran, Louis, ]]                                                                                                          |math.CO cs.CG    |To appear in Graphs and Combinatorics                           |NULL                      |0704.0002|NULL                                              |http://arxiv.org/licenses/nonexclusive-distrib/1.0/|NULL                           |Louis Theran      |Sparsity-certifying Graph Decompositions                                                                                       |2008-12-13 |[{Sat, 31 Mar 2007 02:26:18 GMT, v1}, {Sat, 13 Dec 2008 17:26:00 GMT, v2}]                                    |\n",
            "|  The evolution of Earth-Moon system is described by the dark matter field\\nfluid model proposed in the Meeting of Division of Particle and Field 2004,\\nAmerican Physical Society. The current behavior of the Earth-Moon system agrees\\nwith this model very well and the general pattern of the evolution of the\\nMoon-Earth system described by this model agrees with geological and fossil\\nevidence. The closest distance of the Moon to Earth was about 259000 km at 4.5\\nbillion years ago, which is far beyond the Roche's limit. The result suggests\\nthat the tidal friction may not be the primary cause for the evolution of the\\nEarth-Moon system. The average dark matter field fluid constant derived from\\nEarth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts\\nthat the Mars's rotation is also slowing with the angular acceleration rate\\nabout -4.38 x 10^(-22) rad s^(-2).\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |Hongjun Pan                                                                                                      |[[Pan, Hongjun, ]]                                                                                                                                |physics.gen-ph   |23 pages, 3 figures                                             |NULL                      |0704.0003|NULL                                              |NULL                                               |NULL                           |Hongjun Pan       |The evolution of the Earth-Moon system based on the dark matter field\\n  fluid model                                           |2008-01-13 |[{Sun, 1 Apr 2007 20:46:54 GMT, v1}, {Sat, 8 Dec 2007 23:47:24 GMT, v2}, {Sun, 13 Jan 2008 00:36:28 GMT, v3}] |\n",
            "|  We show that a determinant of Stirling cycle numbers counts unlabeled acyclic\\nsingle-source automata. The proof involves a bijection from these automata to\\ncertain marked lattice paths and a sign-reversing involution to evaluate the\\ndeterminant.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |David Callan                                                                                                     |[[Callan, David, ]]                                                                                                                               |math.CO          |11 pages                                                        |NULL                      |0704.0004|NULL                                              |NULL                                               |NULL                           |David Callan      |A determinant of Stirling cycle numbers counts unlabeled acyclic\\n  single-source automata                                     |2007-05-23 |[{Sat, 31 Mar 2007 03:16:14 GMT, v1}]                                                                         |\n",
            "|  In this paper we show how to compute the $\\Lambda_{\\alpha}$ norm, $\\alpha\\ge\\n0$, using the dyadic grid. This result is a consequence of the description of\\nthe Hardy spaces $H^p(R^N)$ in terms of dyadic and special atoms.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Wael Abu-Shammala and Alberto Torchinsky                                                                         |[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]                                                                                                 |math.CA math.FA  |NULL                                                            |NULL                      |0704.0005|Illinois J. Math. 52 (2008) no.2, 681-689         |NULL                                               |NULL                           |Alberto Torchinsky|From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\alpha}$                                                                           |2013-10-15 |[{Mon, 2 Apr 2007 18:09:58 GMT, v1}]                                                                          |\n",
            "|  We study the two-particle wave function of paired atoms in a Fermi gas with\\ntunable interaction strengths controlled by Feshbach resonance. The Cooper pair\\nwave function is examined for its bosonic characters, which is quantified by\\nthe correction of Bose enhancement factor associated with the creation and\\nannihilation composite particle operators. An example is given for a\\nthree-dimensional uniform gas. Two definitions of Cooper pair wave function are\\nexamined. One of which is chosen to reflect the off-diagonal long range order\\n(ODLRO). Another one corresponds to a pair projection of a BCS state. On the\\nside with negative scattering length, we found that paired atoms described by\\nODLRO are more bosonic than the pair projected definition. It is also found\\nthat at $(k_F a)^{-1} \\ge 1$, both definitions give similar results, where more\\nthan 90% of the atoms occupy the corresponding molecular condensates.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Y. H. Pong and C. K. Law                                                                                         |[[Pong, Y. H., ], [Law, C. K., ]]                                                                                                                 |cond-mat.mes-hall|6 pages, 4 figures, accepted by PRA                             |10.1103/PhysRevA.75.043613|0704.0006|NULL                                              |NULL                                               |NULL                           |Yue Hin Pong      |Bosonic characters of atomic Cooper pairs across resonance                                                                     |2015-05-13 |[{Sat, 31 Mar 2007 04:24:59 GMT, v1}]                                                                         |\n",
            "|  A rather non-standard quantum representation of the canonical commutation\\nrelations of quantum mechanics systems, known as the polymer representation has\\ngained some attention in recent years, due to its possible relation with Planck\\nscale physics. In particular, this approach has been followed in a symmetric\\nsector of loop quantum gravity known as loop quantum cosmology. Here we explore\\ndifferent aspects of the relation between the ordinary Schroedinger theory and\\nthe polymer description. The paper has two parts. In the first one, we derive\\nthe polymer quantum mechanics starting from the ordinary Schroedinger theory\\nand show that the polymer description arises as an appropriate limit. In the\\nsecond part we consider the continuum limit of this theory, namely, the reverse\\nprocess in which one starts from the discrete theory and tries to recover back\\nthe ordinary Schroedinger quantum mechanics. We consider several examples of\\ninterest, including the harmonic oscillator, the free particle and a simple\\ncosmological model.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |Alejandro Corichi, Tatjana Vukasinac and Jose A. Zapata                                                          |[[Corichi, Alejandro, ], [Vukasinac, Tatjana, ], [Zapata, Jose A., ]]                                                                             |gr-qc            |16 pages, no figures. Typos corrected to match published version|10.1103/PhysRevD.76.044016|0704.0007|Phys.Rev.D76:044016,2007                          |NULL                                               |IGPG-07/03-2                   |Alejandro Corichi |Polymer Quantum Mechanics and its Continuum Limit                                                                              |2008-11-26 |[{Sat, 31 Mar 2007 04:27:22 GMT, v1}, {Wed, 22 Aug 2007 22:42:11 GMT, v2}]                                    |\n",
            "|  A general formulation was developed to represent material models for\\napplications in dynamic loading. Numerical methods were devised to calculate\\nresponse to shock and ramp compression, and ramp decompression, generalizing\\nprevious solutions for scalar equations of state. The numerical methods were\\nfound to be flexible and robust, and matched analytic results to a high\\naccuracy. The basic ramp and shock solution methods were coupled to solve for\\ncomposite deformation paths, such as shock-induced impacts, and shock\\ninteractions with a planar interface between different materials. These\\ncalculations capture much of the physics of typical material dynamics\\nexperiments, without requiring spatially-resolving simulations. Example\\ncalculations were made of loading histories in metals, illustrating the effects\\nof plastic work on the temperatures induced in quasi-isentropic and\\nshock-release experiments, and the effect of a phase transition.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Damian C. Swift                                                                                                  |[[Swift, Damian C., ]]                                                                                                                            |cond-mat.mtrl-sci|Minor corrections                                               |10.1063/1.2975338         |0704.0008|Journal of Applied Physics, vol 104, 073536 (2008)|http://arxiv.org/licenses/nonexclusive-distrib/1.0/|LA-UR-07-2051, LLNL-JRNL-410358|Damian Swift      |Numerical solution of shock and ramp compression for general material\\n  properties                                            |2009-02-05 |[{Sat, 31 Mar 2007 04:47:20 GMT, v1}, {Thu, 10 Apr 2008 08:42:28 GMT, v2}, {Tue, 1 Jul 2008 18:54:28 GMT, v3}]|\n",
            "|  We discuss the results from the combined IRAC and MIPS c2d Spitzer Legacy\\nobservations of the Serpens star-forming region. In particular we present a set\\nof criteria for isolating bona fide young stellar objects, YSO's, from the\\nextensive background contamination by extra-galactic objects. We then discuss\\nthe properties of the resulting high confidence set of YSO's. We find 235 such\\nobjects in the 0.85 deg^2 field that was covered with both IRAC and MIPS. An\\nadditional set of 51 lower confidence YSO's outside this area is identified\\nfrom the MIPS data combined with 2MASS photometry. We describe two sets of\\nresults, color-color diagrams to compare our observed source properties with\\nthose of theoretical models for star/disk/envelope systems and our own modeling\\nof the subset of our objects that appear to be star+disks. These objects\\nexhibit a very wide range of disk properties, from many that can be fit with\\nactively accreting disks to some with both passive disks and even possibly\\ndebris disks. We find that the luminosity function of YSO's in Serpens extends\\ndown to at least a few x .001 Lsun or lower for an assumed distance of 260 pc.\\nThe lower limit may be set by our inability to distinguish YSO's from\\nextra-galactic sources more than by the lack of YSO's at very low luminosities.\\nA spatial clustering analysis shows that the nominally less-evolved YSO's are\\nmore highly clustered than the later stages and that the background\\nextra-galactic population can be fit by the same two-point correlation function\\nas seen in other extra-galactic studies. We also present a table of matches\\nbetween several previous infrared and X-ray studies of the Serpens YSO\\npopulation and our Spitzer data set.\\n|Paul Harvey, Bruno Merin, Tracy L. Huard, Luisa M. Rebull, Nicholas\\n  Chapman, Neal J. Evans II, Philip C. Myers|[[Harvey, Paul, ], [Merin, Bruno, ], [Huard, Tracy L., ], [Rebull, Luisa M., ], [Chapman, Nicholas, ], [Evans, Neal J., II], [Myers, Philip C., ]]|astro-ph         |NULL                                                            |10.1086/518646            |0704.0009|Astrophys.J.663:1149-1173,2007                    |NULL                                               |NULL                           |Paul Harvey       |The Spitzer c2d Survey of Large, Nearby, Insterstellar Clouds. IX. The\\n  Serpens YSO Population As Observed With IRAC and MIPS|2010-03-18 |[{Mon, 2 Apr 2007 19:41:34 GMT, v1}]                                                                          |\n",
            "|  Partial cubes are isometric subgraphs of hypercubes. Structures on a graph\\ndefined by means of semicubes, and Djokovi\\'{c}'s and Winkler's relations play\\nan important role in the theory of partial cubes. These structures are employed\\nin the paper to characterize bipartite graphs and partial cubes of arbitrary\\ndimension. New characterizations are established and new proofs of some known\\nresults are given.\\n  The operations of Cartesian product and pasting, and expansion and\\ncontraction processes are utilized in the paper to construct new partial cubes\\nfrom old ones. In particular, the isometric and lattice dimensions of finite\\npartial cubes obtained by means of these operations are calculated.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |Sergei Ovchinnikov                                                                                               |[[Ovchinnikov, Sergei, ]]                                                                                                                         |math.CO          |36 pages, 17 figures                                            |NULL                      |0704.0010|NULL                                              |NULL                                               |NULL                           |Sergei Ovchinnikov|Partial cubes: structures, characterizations, and constructions                                                                |2007-05-23 |[{Sat, 31 Mar 2007 05:10:16 GMT, v1}]                                                                         |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------------------------------------------------------+--------------------------+---------+--------------------------------------------------+---------------------------------------------------+-------------------------------+------------------+-------------------------------------------------------------------------------------------------------------------------------+-----------+--------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 10 rows of the DataFrame with all columns\n",
        "df.limit(10).toPandas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aCQytjTs_Zeo",
        "outputId": "12de3bf9-cff7-4ce0-b936-913441a173f7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            abstract  \\\n",
              "0    A fully differential calculation in perturba...   \n",
              "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
              "2    The evolution of Earth-Moon system is descri...   \n",
              "3    We show that a determinant of Stirling cycle...   \n",
              "4    In this paper we show how to compute the $\\L...   \n",
              "5    We study the two-particle wave function of p...   \n",
              "6    A rather non-standard quantum representation...   \n",
              "7    A general formulation was developed to repre...   \n",
              "8    We discuss the results from the combined IRA...   \n",
              "9    Partial cubes are isometric subgraphs of hyp...   \n",
              "\n",
              "                                             authors  \\\n",
              "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
              "1                    Ileana Streinu and Louis Theran   \n",
              "2                                        Hongjun Pan   \n",
              "3                                       David Callan   \n",
              "4           Wael Abu-Shammala and Alberto Torchinsky   \n",
              "5                           Y. H. Pong and C. K. Law   \n",
              "6  Alejandro Corichi, Tatjana Vukasinac and Jose ...   \n",
              "7                                    Damian C. Swift   \n",
              "8  Paul Harvey, Bruno Merin, Tracy L. Huard, Luis...   \n",
              "9                                 Sergei Ovchinnikov   \n",
              "\n",
              "                                      authors_parsed         categories  \\\n",
              "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...             hep-ph   \n",
              "1           [[Streinu, Ileana, ], [Theran, Louis, ]]      math.CO cs.CG   \n",
              "2                                 [[Pan, Hongjun, ]]     physics.gen-ph   \n",
              "3                                [[Callan, David, ]]            math.CO   \n",
              "4  [[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]    math.CA math.FA   \n",
              "5                  [[Pong, Y. H., ], [Law, C. K., ]]  cond-mat.mes-hall   \n",
              "6  [[Corichi, Alejandro, ], [Vukasinac, Tatjana, ...              gr-qc   \n",
              "7                             [[Swift, Damian C., ]]  cond-mat.mtrl-sci   \n",
              "8  [[Harvey, Paul, ], [Merin, Bruno, ], [Huard, T...           astro-ph   \n",
              "9                          [[Ovchinnikov, Sergei, ]]            math.CO   \n",
              "\n",
              "                                            comments  \\\n",
              "0            37 pages, 15 figures; published version   \n",
              "1              To appear in Graphs and Combinatorics   \n",
              "2                                23 pages, 3 figures   \n",
              "3                                           11 pages   \n",
              "4                                               None   \n",
              "5                6 pages, 4 figures, accepted by PRA   \n",
              "6  16 pages, no figures. Typos corrected to match...   \n",
              "7                                  Minor corrections   \n",
              "8                                               None   \n",
              "9                               36 pages, 17 figures   \n",
              "\n",
              "                          doi         id  \\\n",
              "0  10.1103/PhysRevD.76.013009  0704.0001   \n",
              "1                        None  0704.0002   \n",
              "2                        None  0704.0003   \n",
              "3                        None  0704.0004   \n",
              "4                        None  0704.0005   \n",
              "5  10.1103/PhysRevA.75.043613  0704.0006   \n",
              "6  10.1103/PhysRevD.76.044016  0704.0007   \n",
              "7           10.1063/1.2975338  0704.0008   \n",
              "8              10.1086/518646  0704.0009   \n",
              "9                        None  0704.0010   \n",
              "\n",
              "                                         journal-ref  \\\n",
              "0                           Phys.Rev.D76:013009,2007   \n",
              "1                                               None   \n",
              "2                                               None   \n",
              "3                                               None   \n",
              "4          Illinois J. Math. 52 (2008) no.2, 681-689   \n",
              "5                                               None   \n",
              "6                           Phys.Rev.D76:044016,2007   \n",
              "7  Journal of Applied Physics, vol 104, 073536 (2...   \n",
              "8                     Astrophys.J.663:1149-1173,2007   \n",
              "9                                               None   \n",
              "\n",
              "                                             license  \\\n",
              "0                                               None   \n",
              "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
              "2                                               None   \n",
              "3                                               None   \n",
              "4                                               None   \n",
              "5                                               None   \n",
              "6                                               None   \n",
              "7  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
              "8                                               None   \n",
              "9                                               None   \n",
              "\n",
              "                         report-no           submitter  \\\n",
              "0                 ANL-HEP-PR-07-12      Pavel Nadolsky   \n",
              "1                             None        Louis Theran   \n",
              "2                             None         Hongjun Pan   \n",
              "3                             None        David Callan   \n",
              "4                             None  Alberto Torchinsky   \n",
              "5                             None        Yue Hin Pong   \n",
              "6                     IGPG-07/03-2   Alejandro Corichi   \n",
              "7  LA-UR-07-2051, LLNL-JRNL-410358        Damian Swift   \n",
              "8                             None         Paul Harvey   \n",
              "9                             None  Sergei Ovchinnikov   \n",
              "\n",
              "                                               title update_date  \\\n",
              "0  Calculation of prompt diphoton production cros...  2008-11-26   \n",
              "1           Sparsity-certifying Graph Decompositions  2008-12-13   \n",
              "2  The evolution of the Earth-Moon system based o...  2008-01-13   \n",
              "3  A determinant of Stirling cycle numbers counts...  2007-05-23   \n",
              "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...  2013-10-15   \n",
              "5  Bosonic characters of atomic Cooper pairs acro...  2015-05-13   \n",
              "6  Polymer Quantum Mechanics and its Continuum Limit  2008-11-26   \n",
              "7  Numerical solution of shock and ramp compressi...  2009-02-05   \n",
              "8  The Spitzer c2d Survey of Large, Nearby, Inste...  2010-03-18   \n",
              "9  Partial cubes: structures, characterizations, ...  2007-05-23   \n",
              "\n",
              "                                            versions  \n",
              "0  [(Mon, 2 Apr 2007 19:18:42 GMT, v1), (Tue, 24 ...  \n",
              "1  [(Sat, 31 Mar 2007 02:26:18 GMT, v1), (Sat, 13...  \n",
              "2  [(Sun, 1 Apr 2007 20:46:54 GMT, v1), (Sat, 8 D...  \n",
              "3              [(Sat, 31 Mar 2007 03:16:14 GMT, v1)]  \n",
              "4               [(Mon, 2 Apr 2007 18:09:58 GMT, v1)]  \n",
              "5              [(Sat, 31 Mar 2007 04:24:59 GMT, v1)]  \n",
              "6  [(Sat, 31 Mar 2007 04:27:22 GMT, v1), (Wed, 22...  \n",
              "7  [(Sat, 31 Mar 2007 04:47:20 GMT, v1), (Thu, 10...  \n",
              "8               [(Mon, 2 Apr 2007 19:41:34 GMT, v1)]  \n",
              "9              [(Sat, 31 Mar 2007 05:10:16 GMT, v1)]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e626fc69-ba39-4c64-a37e-02d107377140\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>authors</th>\n",
              "      <th>authors_parsed</th>\n",
              "      <th>categories</th>\n",
              "      <th>comments</th>\n",
              "      <th>doi</th>\n",
              "      <th>id</th>\n",
              "      <th>journal-ref</th>\n",
              "      <th>license</th>\n",
              "      <th>report-no</th>\n",
              "      <th>submitter</th>\n",
              "      <th>title</th>\n",
              "      <th>update_date</th>\n",
              "      <th>versions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A fully differential calculation in perturba...</td>\n",
              "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
              "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
              "      <td>hep-ph</td>\n",
              "      <td>37 pages, 15 figures; published version</td>\n",
              "      <td>10.1103/PhysRevD.76.013009</td>\n",
              "      <td>0704.0001</td>\n",
              "      <td>Phys.Rev.D76:013009,2007</td>\n",
              "      <td>None</td>\n",
              "      <td>ANL-HEP-PR-07-12</td>\n",
              "      <td>Pavel Nadolsky</td>\n",
              "      <td>Calculation of prompt diphoton production cros...</td>\n",
              "      <td>2008-11-26</td>\n",
              "      <td>[(Mon, 2 Apr 2007 19:18:42 GMT, v1), (Tue, 24 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
              "      <td>Ileana Streinu and Louis Theran</td>\n",
              "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
              "      <td>math.CO cs.CG</td>\n",
              "      <td>To appear in Graphs and Combinatorics</td>\n",
              "      <td>None</td>\n",
              "      <td>0704.0002</td>\n",
              "      <td>None</td>\n",
              "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
              "      <td>None</td>\n",
              "      <td>Louis Theran</td>\n",
              "      <td>Sparsity-certifying Graph Decompositions</td>\n",
              "      <td>2008-12-13</td>\n",
              "      <td>[(Sat, 31 Mar 2007 02:26:18 GMT, v1), (Sat, 13...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The evolution of Earth-Moon system is descri...</td>\n",
              "      <td>Hongjun Pan</td>\n",
              "      <td>[[Pan, Hongjun, ]]</td>\n",
              "      <td>physics.gen-ph</td>\n",
              "      <td>23 pages, 3 figures</td>\n",
              "      <td>None</td>\n",
              "      <td>0704.0003</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Hongjun Pan</td>\n",
              "      <td>The evolution of the Earth-Moon system based o...</td>\n",
              "      <td>2008-01-13</td>\n",
              "      <td>[(Sun, 1 Apr 2007 20:46:54 GMT, v1), (Sat, 8 D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>We show that a determinant of Stirling cycle...</td>\n",
              "      <td>David Callan</td>\n",
              "      <td>[[Callan, David, ]]</td>\n",
              "      <td>math.CO</td>\n",
              "      <td>11 pages</td>\n",
              "      <td>None</td>\n",
              "      <td>0704.0004</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>David Callan</td>\n",
              "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
              "      <td>2007-05-23</td>\n",
              "      <td>[(Sat, 31 Mar 2007 03:16:14 GMT, v1)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In this paper we show how to compute the $\\L...</td>\n",
              "      <td>Wael Abu-Shammala and Alberto Torchinsky</td>\n",
              "      <td>[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]</td>\n",
              "      <td>math.CA math.FA</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0704.0005</td>\n",
              "      <td>Illinois J. Math. 52 (2008) no.2, 681-689</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Alberto Torchinsky</td>\n",
              "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
              "      <td>2013-10-15</td>\n",
              "      <td>[(Mon, 2 Apr 2007 18:09:58 GMT, v1)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>We study the two-particle wave function of p...</td>\n",
              "      <td>Y. H. Pong and C. K. Law</td>\n",
              "      <td>[[Pong, Y. H., ], [Law, C. K., ]]</td>\n",
              "      <td>cond-mat.mes-hall</td>\n",
              "      <td>6 pages, 4 figures, accepted by PRA</td>\n",
              "      <td>10.1103/PhysRevA.75.043613</td>\n",
              "      <td>0704.0006</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Yue Hin Pong</td>\n",
              "      <td>Bosonic characters of atomic Cooper pairs acro...</td>\n",
              "      <td>2015-05-13</td>\n",
              "      <td>[(Sat, 31 Mar 2007 04:24:59 GMT, v1)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>A rather non-standard quantum representation...</td>\n",
              "      <td>Alejandro Corichi, Tatjana Vukasinac and Jose ...</td>\n",
              "      <td>[[Corichi, Alejandro, ], [Vukasinac, Tatjana, ...</td>\n",
              "      <td>gr-qc</td>\n",
              "      <td>16 pages, no figures. Typos corrected to match...</td>\n",
              "      <td>10.1103/PhysRevD.76.044016</td>\n",
              "      <td>0704.0007</td>\n",
              "      <td>Phys.Rev.D76:044016,2007</td>\n",
              "      <td>None</td>\n",
              "      <td>IGPG-07/03-2</td>\n",
              "      <td>Alejandro Corichi</td>\n",
              "      <td>Polymer Quantum Mechanics and its Continuum Limit</td>\n",
              "      <td>2008-11-26</td>\n",
              "      <td>[(Sat, 31 Mar 2007 04:27:22 GMT, v1), (Wed, 22...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>A general formulation was developed to repre...</td>\n",
              "      <td>Damian C. Swift</td>\n",
              "      <td>[[Swift, Damian C., ]]</td>\n",
              "      <td>cond-mat.mtrl-sci</td>\n",
              "      <td>Minor corrections</td>\n",
              "      <td>10.1063/1.2975338</td>\n",
              "      <td>0704.0008</td>\n",
              "      <td>Journal of Applied Physics, vol 104, 073536 (2...</td>\n",
              "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
              "      <td>LA-UR-07-2051, LLNL-JRNL-410358</td>\n",
              "      <td>Damian Swift</td>\n",
              "      <td>Numerical solution of shock and ramp compressi...</td>\n",
              "      <td>2009-02-05</td>\n",
              "      <td>[(Sat, 31 Mar 2007 04:47:20 GMT, v1), (Thu, 10...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>We discuss the results from the combined IRA...</td>\n",
              "      <td>Paul Harvey, Bruno Merin, Tracy L. Huard, Luis...</td>\n",
              "      <td>[[Harvey, Paul, ], [Merin, Bruno, ], [Huard, T...</td>\n",
              "      <td>astro-ph</td>\n",
              "      <td>None</td>\n",
              "      <td>10.1086/518646</td>\n",
              "      <td>0704.0009</td>\n",
              "      <td>Astrophys.J.663:1149-1173,2007</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Paul Harvey</td>\n",
              "      <td>The Spitzer c2d Survey of Large, Nearby, Inste...</td>\n",
              "      <td>2010-03-18</td>\n",
              "      <td>[(Mon, 2 Apr 2007 19:41:34 GMT, v1)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Partial cubes are isometric subgraphs of hyp...</td>\n",
              "      <td>Sergei Ovchinnikov</td>\n",
              "      <td>[[Ovchinnikov, Sergei, ]]</td>\n",
              "      <td>math.CO</td>\n",
              "      <td>36 pages, 17 figures</td>\n",
              "      <td>None</td>\n",
              "      <td>0704.0010</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Sergei Ovchinnikov</td>\n",
              "      <td>Partial cubes: structures, characterizations, ...</td>\n",
              "      <td>2007-05-23</td>\n",
              "      <td>[(Sat, 31 Mar 2007 05:10:16 GMT, v1)]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e626fc69-ba39-4c64-a37e-02d107377140')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e626fc69-ba39-4c64-a37e-02d107377140 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e626fc69-ba39-4c64-a37e-02d107377140');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fbabf8ee-2228-4ab4-8573-15c08ab018c3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fbabf8ee-2228-4ab4-8573-15c08ab018c3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fbabf8ee-2228-4ab4-8573-15c08ab018c3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"  We discuss the results from the combined IRAC and MIPS c2d Spitzer Legacy\\nobservations of the Serpens star-forming region. In particular we present a set\\nof criteria for isolating bona fide young stellar objects, YSO's, from the\\nextensive background contamination by extra-galactic objects. We then discuss\\nthe properties of the resulting high confidence set of YSO's. We find 235 such\\nobjects in the 0.85 deg^2 field that was covered with both IRAC and MIPS. An\\nadditional set of 51 lower confidence YSO's outside this area is identified\\nfrom the MIPS data combined with 2MASS photometry. We describe two sets of\\nresults, color-color diagrams to compare our observed source properties with\\nthose of theoretical models for star/disk/envelope systems and our own modeling\\nof the subset of our objects that appear to be star+disks. These objects\\nexhibit a very wide range of disk properties, from many that can be fit with\\nactively accreting disks to some with both passive disks and even possibly\\ndebris disks. We find that the luminosity function of YSO's in Serpens extends\\ndown to at least a few x .001 Lsun or lower for an assumed distance of 260 pc.\\nThe lower limit may be set by our inability to distinguish YSO's from\\nextra-galactic sources more than by the lack of YSO's at very low luminosities.\\nA spatial clustering analysis shows that the nominally less-evolved YSO's are\\nmore highly clustered than the later stages and that the background\\nextra-galactic population can be fit by the same two-point correlation function\\nas seen in other extra-galactic studies. We also present a table of matches\\nbetween several previous infrared and X-ray studies of the Serpens YSO\\npopulation and our Spitzer data set.\\n\",\n          \"  We describe a new algorithm, the $(k,\\\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n\",\n          \"  We study the two-particle wave function of paired atoms in a Fermi gas with\\ntunable interaction strengths controlled by Feshbach resonance. The Cooper pair\\nwave function is examined for its bosonic characters, which is quantified by\\nthe correction of Bose enhancement factor associated with the creation and\\nannihilation composite particle operators. An example is given for a\\nthree-dimensional uniform gas. Two definitions of Cooper pair wave function are\\nexamined. One of which is chosen to reflect the off-diagonal long range order\\n(ODLRO). Another one corresponds to a pair projection of a BCS state. On the\\nside with negative scattering length, we found that paired atoms described by\\nODLRO are more bosonic than the pair projected definition. It is also found\\nthat at $(k_F a)^{-1} \\\\ge 1$, both definitions give similar results, where more\\nthan 90% of the atoms occupy the corresponding molecular condensates.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Paul Harvey, Bruno Merin, Tracy L. Huard, Luisa M. Rebull, Nicholas\\n  Chapman, Neal J. Evans II, Philip C. Myers\",\n          \"Ileana Streinu and Louis Theran\",\n          \"Y. H. Pong and C. K. Law\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors_parsed\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"cond-mat.mtrl-sci\",\n          \"math.CO cs.CG\",\n          \"cond-mat.mes-hall\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"To appear in Graphs and Combinatorics\",\n          \"16 pages, no figures. Typos corrected to match published version\",\n          \"37 pages, 15 figures; published version\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doi\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"10.1103/PhysRevA.75.043613\",\n          \"10.1086/518646\",\n          \"10.1103/PhysRevD.76.044016\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"0704.0009\",\n          \"0704.0002\",\n          \"0704.0006\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"journal-ref\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Illinois J. Math. 52 (2008) no.2, 681-689\",\n          \"Astrophys.J.663:1149-1173,2007\",\n          \"Phys.Rev.D76:044016,2007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"license\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"http://arxiv.org/licenses/nonexclusive-distrib/1.0/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"report-no\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"ANL-HEP-PR-07-12\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"submitter\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Paul Harvey\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The Spitzer c2d Survey of Large, Nearby, Insterstellar Clouds. IX. The\\n  Serpens YSO Population As Observed With IRAC and MIPS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"update_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"2008-12-13\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"versions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check for Duplicate Rows**"
      ],
      "metadata": {
        "id": "c_diSV8xAQz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total rows and distinct rows\n",
        "total_rows = df.count()\n",
        "distinct_rows = df.distinct().count()\n",
        "\n",
        "print(f\"Total Rows: {total_rows}\")\n",
        "print(f\"Distinct Rows: {distinct_rows}\")\n",
        "\n",
        "# Check for duplicates\n",
        "duplicate_rows = total_rows - distinct_rows\n",
        "print(f\"Number of duplicate rows: {duplicate_rows}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ib7ZP6fAQDT",
        "outputId": "4c14d999-cca7-409c-e46f-841f4f302dde"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows: 2635119\n",
            "Distinct Rows: 2635109\n",
            "Number of duplicate rows: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check for Missing Values**\n",
        "\n",
        "checking for missing values column-wise:"
      ],
      "metadata": {
        "id": "bk2IgMVqFFlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing values for each column\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "missing_values = df.select(\n",
        "    [sum(col(column).isNull().cast(\"int\")).alias(column) for column in df.columns]\n",
        ")\n",
        "\n",
        "missing_values.show()  # Show the count of missing values for each column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K6G0mOdsv3O",
        "outputId": "0cfb2ec5-54fa-490a-efbf-5cffadca52b8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+--------------+----------+--------+-------+---+-----------+-------+---------+---------+-----+-----------+--------+\n",
            "|abstract|authors|authors_parsed|categories|comments|    doi| id|journal-ref|license|report-no|submitter|title|update_date|versions|\n",
            "+--------+-------+--------------+----------+--------+-------+---+-----------+-------+---------+---------+-----+-----------+--------+\n",
            "|       0|      0|             0|         0|  672050|1419097|  0|    1761899| 452795|  2451733|    15189|    0|          0|       0|\n",
            "+--------+-------+--------------+----------+--------+-------+---+-----------+-------+---------+---------+-----+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop Duplicates and Rows with Missing Values**\n",
        "\n",
        "Using PySpark to clean the data:"
      ],
      "metadata": {
        "id": "NIwt3dmPHUcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate rows\n",
        "df = df.dropDuplicates()\n",
        "\n",
        "# Remove rows with any missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Check the new row count\n",
        "print(f\"DataFrame after cleaning: {df.count()} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XM4QNMHCumG",
        "outputId": "9920c69c-955f-4d70-e4bc-ef7e63990851"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after cleaning: 41198 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Exploration:**\n",
        "\n",
        "Before jumping into ML tasks, explore the data to understand its structure and distribution.\n",
        "\n",
        "**Show the Schema:**\n",
        "\n",
        "Check the data types of the columns."
      ],
      "metadata": {
        "id": "DF7JrY0rK-yI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f23qBCYRCuo1",
        "outputId": "9355eb9c-0dcc-40a2-dfd1-3611ddf36cdb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- abstract: string (nullable = true)\n",
            " |-- authors: string (nullable = true)\n",
            " |-- authors_parsed: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- categories: string (nullable = true)\n",
            " |-- comments: string (nullable = true)\n",
            " |-- doi: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- journal-ref: string (nullable = true)\n",
            " |-- license: string (nullable = true)\n",
            " |-- report-no: string (nullable = true)\n",
            " |-- submitter: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- update_date: string (nullable = true)\n",
            " |-- versions: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- created: string (nullable = true)\n",
            " |    |    |-- version: string (nullable = true)\n",
            "\n",
            "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------+--------------------+-----------+--------------------+\n",
            "|            abstract|             authors|      authors_parsed|categories|            comments|                 doi|       id|         journal-ref|             license|           report-no|     submitter|               title|update_date|            versions|\n",
            "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------+--------------------+-----------+--------------------+\n",
            "|  We present a un...|Brian P. Dolan, I...|[[Dolan, Brian P....|    hep-th|25 pages, typeset...|10.1088/1126-6708...|0711.1347|   JHEP0803:029,2008|http://arxiv.org/...|      DIAS-STP-07-19|   Brian Dolan|A universal Dirac...| 2008-11-26|[{Thu, 8 Nov 2007...|\n",
            "|  We solve the la...|           Dong Wang|    [[Wang, Dong, ]]|   math.PR|Published in at h...|   10.1214/08-AOP432|0711.2722|Annals of Probabi...|http://arxiv.org/...|      IMS-AOP-AOP432|     Dong Wang|The largest sampl...| 2009-10-12|[{Mon, 19 Nov 200...|\n",
            "|  A model has bee...|Yonatan Kahn, Mic...|[[Kahn, Yonatan, ...|    hep-ph|4 pages, 2 figure...|10.1103/PhysRevD....|0712.0007|Phys.Rev.D78:1150...|http://arxiv.org/...|     NUHEP-EXP/07-12|Tim M. P. Tait|Enhanced Rare Pio...| 2012-05-25|[{Fri, 30 Nov 200...|\n",
            "|  This paper is c...|        Oliver Rinne| [[Rinne, Oliver, ]]|     gr-qc|22 pages, 8 figur...|10.1088/0264-9381...|0802.3791|Class.Quant.Grav....|http://arxiv.org/...|       DAMTP-2008-14|  Oliver Rinne|Constrained evolu...| 2008-11-26|[{Tue, 26 Feb 200...|\n",
            "|  We discuss a ne...|Huaiyu Duan (INT,...|[[Duan, Huaiyu, ,...|  astro-ph|25 pages, 6 figur...|10.1088/1749-4699...|0803.3650|Comput.Sci.Dis.1:...|http://arxiv.org/...|INT PUB 08-05, LA...|   Huaiyu Duan|Simulating nonlin...| 2009-06-09|[{Wed, 26 Mar 200...|\n",
            "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------+--------------------+-----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary Statistics:**\n",
        "\n",
        "Get a statistical summary of numerical columns."
      ],
      "metadata": {
        "id": "pHBOpAwyLbzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjEsIj72CurM",
        "outputId": "601ea884-ed14-4b86-e75c-df79d02a4c10"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------------+---------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+-----------+\n",
            "|summary|            abstract|             authors|     categories|            comments|                 doi|                id|         journal-ref|             license|           report-no|       submitter|               title|update_date|\n",
            "+-------+--------------------+--------------------+---------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+-----------+\n",
            "|  count|               41198|               41198|          41198|               41198|               41198|             41198|               41198|               41198|               41198|           41198|               41198|      41198|\n",
            "|   mean|                NULL|                NULL|           NULL|                19.3|                NULL|1462.7080176051954|             2021.28|                NULL|1.189093493516714...|            NULL|                NULL|       NULL|\n",
            "| stddev|                NULL|                NULL|           NULL|  10.078028907810626|                NULL|473.82825113687784|  1.8147543451755372|                NULL|2.045455633782570...|            NULL|                NULL|       NULL|\n",
            "|    min|  \"As you are wel...|(1) A. Lundgren, ...|       astro-ph|\"38 pages. v3: im...|10.1002/1521-3978...|         0704.0008|\"Breakdown of the...|http://arxiv.org/...|( Volume: 69, Iss...|          A Hart|\"Anomaly\" in n=in...| 2008-02-22|\n",
            "|    max|  { {\\bf Backgrou...|{\\O}yvind Almelid...|stat.OT stat.ME|   ~4 Phys Rev pages|https://doi.org/1...|  quant-ph/9805012|Épijournal de Géo...|http://creativeco...|{IFIC/19-07; FERM...|{\\L}ukasz Lenart|{\\pi} Meson-Octet...| 2024-12-25|\n",
            "+-------+--------------------+--------------------+---------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count Unique Categories:**\n",
        "\n",
        "Explore the categories column to see how many unique categories exist."
      ],
      "metadata": {
        "id": "Jg-2o1SMND5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories_count = df.select(\"categories\").distinct().count()\n",
        "print(f\"Number of unique categories: {categories_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEmkXhBnCuty",
        "outputId": "a82d51ed-d69d-4b70-c64f-79022f746d29"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique categories: 3528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top Categories:**\n",
        "\n",
        "Show the most frequent categories."
      ],
      "metadata": {
        "id": "LpdlXmmMO32j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count\n",
        "df.groupBy(\"categories\").agg(count(\"*\").alias(\"count\")).orderBy(col(\"count\").desc()).show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLbDZOjUCuwU",
        "outputId": "0d8d2799-d7ad-4e5e-f3e7-8974fa0de34f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----+\n",
            "|     categories|count|\n",
            "+---------------+-----+\n",
            "|         hep-ph| 5008|\n",
            "|         hep-ex| 3961|\n",
            "|         hep-th| 3451|\n",
            "|  hep-ph hep-ex| 1981|\n",
            "|math.ST stat.TH| 1107|\n",
            "|   hep-th gr-qc|  931|\n",
            "|        math.PR|  922|\n",
            "|  hep-ph hep-th|  619|\n",
            "|        stat.AP|  596|\n",
            "|  hep-th hep-ph|  589|\n",
            "+---------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the code:**\n",
        "--from pyspark.sql.functions import col, count\n",
        "\n",
        "--This line imports specific functions from the pyspark.sql.functions module. col is used to access a DataFrame's column, and count is used to count rows.\n",
        "\n",
        "--df.groupBy(\"categories\").agg(count(\"*\").alias(\"count\")).orderBy(col(\"count\").desc()).show(10)\n",
        "\n",
        "\n",
        "--df.groupBy(\"categories\"): This groups the DataFrame df by the values in the \"categories\" column.\n",
        "\n",
        "--.agg(count(\"*\").alias(\"count\")): This aggregates the grouped DataFrame by counting the rows in each group and naming this count column as \"count\".\n",
        "\n",
        "--.orderBy(col(\"count\").desc()): This orders the aggregated results in descending order based on the \"count\" column.\n",
        "\n",
        "--.show(10): This displays the first 10 rows of the final result.\n",
        "\n",
        "\n",
        "**Understanding the Dot (.)**\n",
        "In Python, the dot (.) operator is used for accessing methods and properties of an object. It's a way of \"chaining\" methods together to perform complex operations step by step. For instance:\n",
        "\n",
        "--df.groupBy(\"categories\") calls the groupBy method on the DataFrame df.\n",
        "\n",
        "--.agg(count(\"*\").alias(\"count\")) then calls the agg method on the result of the previous groupBy method.\n",
        "\n",
        "--And so on...\n",
        "\n",
        "**When to Use the Dot**\n",
        "\n",
        "Use the dot (.) whenever you need to call a method or access a property of an object. It's essential in object-oriented programming to invoke functions on instances of classes or to retrieve attributes of objects.\n"
      ],
      "metadata": {
        "id": "EkAWRxcnZCXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What's NEXT?**\n",
        "In big datasets like this, if we try to process every column, it could consume a lot of memory and crash our system.\n",
        "\n",
        "**Key Points to Consider:**\n",
        "\n",
        "**Select Relevant Columns:**\n",
        "\n",
        "For ML, we don't need to use all columns. Focus on columns that contain valuable information for our analysis.\n",
        "\n",
        "--Example: title, abstract, and categories are often the most useful:\n",
        "\n",
        "--title/abstract: Used to extract text-based features for predictive modeling.\n",
        "\n",
        "--categories: Often serves as the target variable for classification tasks.\n",
        "\n",
        "**Avoid Wasting Resources:**\n",
        "\n",
        "--Columns like id, authors, license, or submitter may not contribute much to our analysis and can be ignored.\n",
        "\n",
        "**Efficient Feature Selection:**\n",
        "\n",
        "Instead of transforming all string columns, focus on one or two text-heavy columns (like abstract) to generate features.\n",
        "\n",
        "This saves memory and avoids crashing.\n"
      ],
      "metadata": {
        "id": "D8xxO2ULU426"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-by-Step Instructions for ML Analysis:**"
      ],
      "metadata": {
        "id": "iEzFjRO5Vgnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select Only Useful Columns**\n",
        "\n",
        "Filter the DataFrame to keep only abstract, title, and categories:"
      ],
      "metadata": {
        "id": "a-8QbBLSCvCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.select(\"abstract\", \"title\", \"categories\")\n",
        "print(f\"Columns selected: {df.columns}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzekl3FlCvGL",
        "outputId": "8b7e83dd-9004-4135-e08b-ac6718dfd086"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns selected: ['abstract', 'title', 'categories']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess Text Columns**\n",
        "\n",
        "Transform abstract or title into numeric features for ML analysis.\n",
        "\n",
        "Tokenize Text: Split the text into words."
      ],
      "metadata": {
        "id": "ETImYti8Vxrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"abstract\", outputCol=\"tokens\")\n",
        "df = tokenizer.transform(df)\n",
        "df.select(\"abstract\", \"tokens\").show(5, truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6MUeCbkCvIh",
        "outputId": "a6451e5f-8913-4fa9-9b3e-19d0a2979088"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                          abstract|                                            tokens|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|  We present a universal Dirac operator for non...|[, , we, present, a, universal, dirac, operator...|\n",
            "|  We solve the largest sample eigenvalue distri...|[, , we, solve, the, largest, sample, eigenvalu...|\n",
            "|  A model has been proposed in which neutral sc...|[, , a, model, has, been, proposed, in, which, ...|\n",
            "|  This paper is concerned with the Einstein equ...|[, , this, paper, is, concerned, with, the, ein...|\n",
            "|  We discuss a new kind of astrophysical transp...|[, , we, discuss, a, new, kind, of, astrophysic...|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert Words into Numeric Features:**\n",
        "\n",
        " Use HashingTF or TF-IDF to convert words into numeric vectors."
      ],
      "metadata": {
        "id": "5t5i3T7kV-WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"features\", numFeatures=1000)\n",
        "df = hashingTF.transform(df)\n",
        "df.select(\"abstract\", \"features\").show(5, truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eABu1SeNsv5J",
        "outputId": "e42cc718-a8e7-431e-887f-988c28fceac9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                          abstract|                                          features|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|  We present a universal Dirac operator for non...|(1000,[17,30,39,93,121,130,143,165,173,195,199,...|\n",
            "|  We solve the largest sample eigenvalue distri...|(1000,[17,40,46,99,102,162,179,209,214,249,280,...|\n",
            "|  A model has been proposed in which neutral sc...|(1000,[17,29,58,66,69,78,80,100,102,106,133,157...|\n",
            "|  This paper is concerned with the Einstein equ...|(1000,[7,17,19,47,48,78,115,116,133,135,137,157...|\n",
            "|  We discuss a new kind of astrophysical transp...|(1000,[4,17,44,54,63,66,76,80,85,95,112,117,130...|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Breakdown**\n",
        "\n",
        "--from pyspark.ml.feature import HashingTF\n",
        "\n",
        "--This line imports the HashingTF class from the pyspark.ml.feature module. HashingTF is a feature transformer that converts a collection of tokens into a fixed-length feature vector using the hashing trick.\n",
        "\n",
        "--hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"features\", numFeatures=1000)\n",
        "\n",
        "--This creates an instance of HashingTF with the following parameters:\n",
        "\n",
        "--inputCol=\"tokens\": The column name for the input tokens.\n",
        "\n",
        "--outputCol=\"features\":\n",
        "\n",
        "The column name for the output feature vector.\n",
        "\n",
        "--numFeatures=1000: The number of features (dimensionality of the feature vector).\n",
        "\n",
        "---df = hashingTF.transform(df)\n",
        "\n",
        "--This transforms the DataFrame df by applying the HashingTF transformer to it. The transformation converts the \"tokens\" column into a \"features\" column containing the hashed feature vectors.\n",
        "\n",
        "---df.select(\"abstract\", \"features\").show(5, truncate=50)\n",
        "\n",
        "--This selects the \"abstract\" and \"features\" columns from the DataFrame and displays the first 5 rows. The truncate=50 parameter truncates long strings in the output to 50 characters for easier readability.\n",
        "\n"
      ],
      "metadata": {
        "id": "XtEnNc0qct0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encode Target Column (categories)**\n",
        "\n",
        "Convert categories into numeric labels for classification."
      ],
      "metadata": {
        "id": "hl28wljnXvI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"categories\", outputCol=\"label\")\n",
        "df = indexer.fit(df).transform(df)\n",
        "df.select(\"categories\", \"label\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Auakqfuusv73",
        "outputId": "e84a631d-8e61-4a25-8669-b6402127a41f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|categories|label|\n",
            "+----------+-----+\n",
            "|    hep-th|  2.0|\n",
            "|   math.PR|  6.0|\n",
            "|    hep-ph|  0.0|\n",
            "|     gr-qc| 11.0|\n",
            "|  astro-ph| 47.0|\n",
            "+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Breakdown**\n",
        "\n",
        "---from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "--This line imports the StringIndexer class from the pyspark.ml.feature module. StringIndexer is a feature transformer that encodes a string column of labels to a column of label indices.\n",
        "\n",
        "---indexer = StringIndexer(inputCol=\"categories\", outputCol=\"label\")\n",
        "\n",
        "\n",
        "--This creates an instance of StringIndexer with the following parameters:\n",
        "\n",
        "--inputCol=\"categories\": The column name for the input string values.\n",
        "\n",
        "--outputCol=\"label\": The column name for the output indexed values.\n",
        "\n",
        "---df = indexer.fit(df).transform(df)\n",
        "\n",
        "--This does two things:\n",
        "\n",
        "--indexer.fit(df): Fits the StringIndexer model on the DataFrame df. During this fitting process, StringIndexer learns the mapping from string labels to integer indices.\n",
        "\n",
        "--.transform(df): Transforms the DataFrame df using the fitted StringIndexer model, adding a new column \"label\" with the indexed values.\n",
        "\n",
        "---df.select(\"categories\", \"label\").show(5)\n",
        "\n",
        "\n",
        "--This selects the \"categories\" and \"label\" columns from the DataFrame and displays the first 5 rows.\n"
      ],
      "metadata": {
        "id": "7ux6vK7SfU0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Data**\n",
        "\n",
        "Split the data into training (80%) and testing (20%) datasets."
      ],
      "metadata": {
        "id": "4gFGuAc1ZvNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "print(f\"Training Data: {train_data.count()} rows\")\n",
        "print(f\"Testing Data: {test_data.count()} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0ZEXv73WIRN",
        "outputId": "9751df16-e0ff-40c8-cb03-4ecca371e720"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data: 33096 rows\n",
            "Testing Data: 8102 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: I cannot run the following scripts in the local machine due to the large size of data....**\n",
        "\n",
        "**Suggestions:**\n",
        "\n",
        "--Train on a smaller sample of the data ( 10% of data) to verify if the process is working as expected.\n",
        "\n",
        "sample_train_data = train_data.sample(fraction=0.1, seed=42)  # Use 10% of the data\n",
        "\n",
        "model = lr.fit(sample_train_data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iksHbFRYPr1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train a Simple Model**\n",
        "\n",
        "Train a classification model (e.g., Logistic Regression) using the features and labels."
      ],
      "metadata": {
        "id": "f9txk1u6dTnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "model = lr.fit(train_data)\n",
        "print(\"Model training completed!\")"
      ],
      "metadata": {
        "id": "tyO9KT8wWIT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the Model**\n",
        "\n",
        "Use the test data to check how well the model performs."
      ],
      "metadata": {
        "id": "kIKbOFLVhJyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(test_data)\n",
        "predictions.select(\"label\", \"prediction\", \"probability\").show(10)\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "mP-j5pedWIWe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfhE8D8VZ3cpEooADxjflU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}